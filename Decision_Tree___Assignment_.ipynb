{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Decision Tree | Assignment"
      ],
      "metadata": {
        "id": "Ti9dXU-LyCmZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 1:  What is a Decision Tree, and how does it work in the context of classification?\n",
        "\n",
        "A decision tree is a supervised learning algorithm used for both classification and regression tasks. It has a hierarchical tree structure which consists of a root node, branches, internal nodes and leaf nodes. It works like a flowchart help to make decisions step by step where:\n",
        "\n",
        "- Internal nodes represent attribute tests\n",
        "- Branches represent attribute values\n",
        "- Leaf nodes represent final decisions or predictions.\n",
        "\n",
        "Decision trees are widely used due to their interpretability, flexibility and low preprocessing needs.\n",
        "\n",
        "\n",
        "# How a Decision Tree Works for Classification:\n",
        "\n",
        "The process works like a series of questions and answers:\n",
        "\n",
        "- Start with the Root Node: The entire dataset begins at the top, called the root node.\n",
        "\n",
        "- Feature Selection (Splitting): The algorithm evaluates all available features to determine which one best splits the data into the most homogeneous groups with respect to the target variable (the class label). Common metrics used for this evaluation are Information Gain or the Gini Index.\n",
        "\n",
        "- Recursive Partitioning: The process of splitting the data is repeated recursively for each new, resulting subset (child node). The goal at each step is to maximize the \"purity\" of the nodes, meaning that each resulting node contains as many instances of a single class as possible.\n",
        "\n",
        "- Creating Decision (Internal) Nodes: Each split creates a decision node, which represents a test on an attribute (e.g., \"Is the email content spam?\").\n",
        "\n",
        "- Reaching Leaf Nodes: The process stops when a predefined criterion is met, such as when the nodes are pure (all data points belong to the same class), when a certain tree depth is reached, or when the number of data points in a node is below a minimum threshold. The final nodes that are not split further are called leaf nodes.\n",
        "\n",
        "- Making a Prediction: Each leaf node represents a final class label (e.g., \"Spam\" or \"Not Spam\"). To classify a new data point, you start at the root node and follow the path down the tree by answering the questions at each decision node until you reach a leaf node, which gives the classification.\n",
        "\n",
        "In essence, a decision tree mimics human decision-making by creating a set of IF-THEN rules to classify data."
      ],
      "metadata": {
        "id": "qfKZNw49yCjA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 2: Explain the concepts of Gini Impurity and Entropy as impurity measures. How do they impact the splits in a Decision Tree?\n",
        "\n",
        "\n",
        "\n",
        "- Gini Impurity and Entropy are impurity measures used in decision trees to determine the best split by quantifying the disorder in a dataset. Gini Impurity measures the probability of misclassifying a random sample, aiming for a low score, while Entropy measures the disorder or uncertainty, seeking to reduce it to zero. The impact on splits is that the algorithm chooses the feature that results in the greatest reduction of either Gini Impurity or Entropy after the split, as this indicates a more \"pure\" or \"certain\" child node.\n",
        "\n",
        "## Gini Impurity\n",
        "\n",
        "- Concept: Measures the likelihood of a randomly chosen element being incorrectly classified if it were randomly labeled according to the class distribution of the node.\n",
        "\n",
        "- Calculation: For a node with k classes, the formula is $Gini = 1 - \\sum_{i=1}^{n} p_i^2$\n",
        "\n",
        "where \\(p_i^$) is the proportion of elements belonging to class \\(i\\).\n",
        "\n",
        "- Goal: The goal is to achieve a Gini Impurity of 0, which indicates a \"pure\" node where all elements belong to the same class.\n",
        "\n",
        "- Impact on Splits: A split is considered good if it results in a lower Gini Impurity in the child nodes compared to the parent node. The algorithm selects the split that maximizes the reduction in Gini Impurity (or \"Gini Gain\").\n",
        "\n",
        "## Entropy\n",
        "\n",
        "- Concept: Measures the disorder or randomness in a dataset. A higher entropy value indicates greater uncertainty.\n",
        "\n",
        "- Calculation: The formula for entropy is \\(Entropy=-\\sum _{i=1}^{k}p_{i}\\log _{2}(p_{i})\\), where \\(p_{i}\\) is the proportion of elements belonging to class \\(i\\).\n",
        "\n",
        "- Goal: To reduce the entropy to 0, which signifies a \"pure\" node with no uncertainty about the class labels.\n",
        "\n",
        "- Impact on Splits: The algorithm chooses the feature and split that results in the greatest reduction of entropy. This reduction is known as \"Information Gain\".\n",
        "\n",
        "- Speed: Computing entropy involves logarithmic calculations, which can be computationally more expensive than Gini Impurity.\n",
        "\n",
        "\n",
        "##  How they impact splits in a Decision Tree Scoring potential splits:\n",
        "\n",
        "For each potential split (e.g., splitting on a specific feature with a certain threshold), the impurity of the resulting child nodes is calculated using either Gini Impurity or Entropy.Calculating the gain: The impurity reduction is calculated for that split. For example, the Information Gain from a split is the parent node's impurity minus the weighted average impurity of the child nodes.Choosing the best split: The algorithm compares the impurity reduction for all possible splits. It then selects the split that results in the maximum impurity reduction (either Gini Gain or Information Gain).Iterative process: This process is repeated for each new node until a stopping criterion is met, such as reaching a maximum depth or a minimum number of samples in a node."
      ],
      "metadata": {
        "id": "lz0HE0xnyCgI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Question 3: What is the difference between Pre-Pruning and Post-Pruning in Decision Trees? Give one practical advantage of using each.\n",
        "\n",
        "- **Pre-pruning** stops a decision tree from growing by setting criteria like maximum depth, while post-pruning removes branches from a fully grown tree. A practical advantage of **pre-pruning** is its computational efficiency, as it avoids the cost of building a large, unnecessary tree, while **post-pruning's** advantage is its potentially higher accuracy, since it can make more informed decisions by evaluating the full tree structure before making cuts.\n",
        "\n",
        "## Pre-pruning\n",
        "\n",
        "- How it works: Prevents the tree from growing excessively by stopping the splitting process early based on predefined conditions.\n",
        "\n",
        "- Conditions: These can include setting a maximum tree depth, a minimum number of samples required to split a node, or a minimum information gain.\n",
        "\n",
        "- Practical advantage: Computational efficiency. It is more computationally efficient because it prevents the algorithm from building a full tree that might later be pruned, saving time and resources during training, especially for large datasets.\n",
        "\n",
        "## Post-pruning\n",
        "- How it works: A full decision tree is first constructed, and then branches are removed if they are deemed insignificant or if pruning improves the overall model performance.\n",
        "\n",
        "- Conditions: A common approach is to use a validation set to evaluate the impact of removing a branch on the tree's accuracy.\n",
        "\n",
        "- Practical advantage: Potentially higher accuracy. It can lead to better pruning decisions because it analyzes the complete tree structure, allowing for more robust evaluation of a branch's impact on performance and potentially leading to a more generalized model compared to pre-pruning which might stop too early.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "   |  Feature        | Pre-Pruning               |    Post-Pruning                      |\n",
        "   | -------------- | ------------------------- | --------------------------------- |\n",
        "   | When applied   | During tree growth        | After full tree is grown          |\n",
        "   | Goal           | Prevent overfitting early | Remove overfitting after building |\n",
        "   | Complexity     | Lower                     | Higher                            |\n",
        "   | Main advantage | Faster, cheaper training  | Better predictive performance     |\n"
      ],
      "metadata": {
        "id": "Eh8V-Ix0yCdB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pre-pruning\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "X, y = load_iris(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Pre-pruned tree\n",
        "pre_pruned_tree = DecisionTreeClassifier(\n",
        "    max_depth=3,           # limit depth\n",
        "    min_samples_split=5,   # require at least 5 samples to split\n",
        "    min_samples_leaf=2,    # require at least 2 samples per leaf\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "pre_pruned_tree.fit(X_train, y_train)\n",
        "pred = pre_pruned_tree.predict(X_test)\n",
        "print(\"Pre-Pruned Accuracy:\", accuracy_score(y_test, pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "59taNNFPNFbN",
        "outputId": "a15c9f60-8e35-4ac7-a720-c356818df94e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pre-Pruned Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Post-pruning\n",
        "\n",
        "#Step 1: Train tree fully\n",
        "\n",
        "tree = DecisionTreeClassifier(random_state=42)\n",
        "tree.fit(X_train, y_train)\n",
        "\n",
        "#Step 2: Get effective alphas for pruning\n",
        "\n",
        "path = tree.cost_complexity_pruning_path(X_train, y_train)\n",
        "ccp_alphas = path.ccp_alphas\n",
        "\n",
        "# Step 3: Train several pruned trees and choose the best\n",
        "\n",
        "trees = []\n",
        "for alpha in ccp_alphas:\n",
        "    t = DecisionTreeClassifier(random_state=42, ccp_alpha=alpha)\n",
        "    t.fit(X_train, y_train)\n",
        "    trees.append(t)\n",
        "\n",
        "#Step 4: Pick the best based on validation/test accuracy\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "accuracies = [accuracy_score(y_test, t.predict(X_test)) for t in trees]\n",
        "best_tree = trees[np.argmax(accuracies)]\n",
        "\n",
        "print(\"Best post-pruned accuracy:\", max(accuracies))\n",
        "print(\"Best alpha:\", ccp_alphas[np.argmax(accuracies)])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pyVx_5TZN_p2",
        "outputId": "83be9bbc-9cc7-4985-cf5f-c7f898365dbe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best post-pruned accuracy: 1.0\n",
            "Best alpha: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 4: What is Information Gain in Decision Trees, and why is it important for choosing the best split?\n",
        "\n",
        "- Information Gain (IG) is a metric used in Decision Trees to measure how much a feature helps reduce uncertainty (impurity) in the target variable when splitting the data.\n",
        "\n",
        "- Information Gain = Reduction in impurity after a dataset is split using a feature.\n",
        "\n",
        "## Formula:\n",
        "\n",
        "$$\n",
        "IG = Impurity(parent) - \\sum_{i=1}^{n} \\frac{n_i}{n} \\cdot Impurity(child_i)\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "- $Impurity$ = Entropy or Gini\n",
        "- $n$ = number of samples in parent\n",
        "- $n_i$ = number of samples in child node\n",
        "- $k$ = number of child nodes\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "####Information gain is important for choosing the best split in a decision tree because it quantifies how much a feature helps in separating the data into purer, more homogeneous groups. The feature with the highest information gain is selected as the split because it leads to the largest reduction in uncertainty (entropy) and improves the model's predictive accuracy.\n",
        "\n",
        "- Measures impurity reduction: Information gain measures the decrease in entropy, a metric that quantifies impurity or uncertainty in a dataset. A higher information gain means the split based on that feature results in child nodes that are more homogeneous with respect to the target variable.\n",
        "- Selects the best feature: To build an effective decision tree, the algorithm selects the feature that provides the most information gain at each node to make the split. This is done iteratively, with the feature that has the highest information gain being chosen to create the sub-nodes.\n",
        "- Improves model accuracy: By consistently choosing splits that create the purest possible groups, information gain helps build a more accurate and efficient decision tree. It helps the model make better decisions by reducing the uncertainty about the final outcome with each split.\n",
        "- Handles different data types: Information gain can be used for both categorical and numerical features. For numerical features, it calculates the information gain for different possible split points (thresholds) and chooses the one with the highest gain.\n",
        "\n"
      ],
      "metadata": {
        "id": "FUd2QWMeyCaJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 5: What are some common real-world applications of Decision Trees, and what are their main advantages and limitations?\n",
        "\n",
        "###Common Real-World Applications of Decision Trees:\n",
        "\n",
        "- Healthcare: Assisting in medical diagnosis by predicting diseases based on patient data like blood pressure and glucose levels.\n",
        "\n",
        "- Finance: Evaluating creditworthiness for loan applications, conducting risk assessment, and detecting fraudulent transactions.\n",
        "\n",
        "- Marketing: Predicting customer behavior, such as identifying customers likely to churn or respond to a marketing campaign, and segmenting customers based on their behavior and demographics.\n",
        "\n",
        "- Business: Strategic planning, resource allocation, and evaluating new product launches or market expansions.\n",
        "\n",
        "- Education: Predicting exam results based on factors like attendance and past grades to identify at-risk students.\n",
        "\n",
        "- Automated systems: Guiding users through automated telephone systems or other interactive processes.\n",
        "\n",
        "### Advantages:\n",
        "\n",
        "- Easy to understand: The visual, flowchart-like structure is simple to comprehend, even for those without a deep analytical background.\n",
        "\n",
        "- Handles different data types: They can be used with both numerical (e.g., income) and categorical (e.g., gender) data.\n",
        "\n",
        "- Non-linear relationships: They can easily capture and model nonlinear relationships in data.\n",
        "\n",
        "- Feature selection: They can help identify the most important variables for a problem, as they show the relationships between variables.\n",
        "\n",
        "- Transparency: Each decision node can be traced, making the reasoning behind a prediction clear, which is crucial for gaining trust in fields like finance and healthcare.\n",
        "\n",
        "### Limitations:\n",
        "\n",
        "- Prone to overfitting: They can create overly complex trees that do not generalize well to new, unseen data.\n",
        "\n",
        "- Instability: Small variations in the data can lead to the creation of a completely different tree.\n",
        "\n",
        "- Bias: Decision trees can be biased toward features with more levels.\n",
        "\n",
        "- Not ideal for regression: While they can be used for regression, they often perform better with classification tasks, as the predictions can be biased towards the most frequent class."
      ],
      "metadata": {
        "id": "LjBZul6VyCXK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 6:   Write a Python program to:\n",
        "- Load the Iris Dataset\n",
        "- Train a Decision Tree Classifier using the Gini criterion\n",
        "- Print the model’s accuracy and feature importances"
      ],
      "metadata": {
        "id": "FQrzNBJWyCT5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Load the Iris Dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# 2. Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 3. Train Decision Tree Classifier with Gini criterion\n",
        "model = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# 4. Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# 5. Print accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Model Accuracy:\", accuracy)\n",
        "\n",
        "# 6. Print feature importances\n",
        "print(\"Feature Importances:\")\n",
        "for feature, importance in zip(iris.feature_names, model.feature_importances_):\n",
        "    print(f\"{feature}: {importance:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xdn2Vzjqe5xE",
        "outputId": "9cec7898-145c-4e5f-9c3d-d7f645dd386a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 1.0\n",
            "Feature Importances:\n",
            "sepal length (cm): 0.0000\n",
            "sepal width (cm): 0.0167\n",
            "petal length (cm): 0.9061\n",
            "petal width (cm): 0.0772\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 7:  Write a Python program to:\n",
        "- Load the Iris Dataset\n",
        "- Train a Decision Tree Classifier with max_depth=3 and compare its accuracy to\n",
        "a fully-grown tree."
      ],
      "metadata": {
        "id": "zZbq-Y0LyCNa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Decision Tree with max_depth = 3\n",
        "tree_limited = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
        "tree_limited.fit(X_train, y_train)\n",
        "pred_limited = tree_limited.predict(X_test)\n",
        "acc_limited = accuracy_score(y_test, pred_limited)\n",
        "\n",
        "# Fully-grown Decision Tree\n",
        "tree_full = DecisionTreeClassifier(random_state=42)\n",
        "tree_full.fit(X_train, y_train)\n",
        "pred_full = tree_full.predict(X_test)\n",
        "acc_full = accuracy_score(y_test, pred_full)\n",
        "\n",
        "acc_limited, acc_full\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Og_BFE8qgejs",
        "outputId": "efd07610-bc40-4ffe-a108-0904857be9a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1.0, 1.0)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 8: Write a Python program to:\n",
        "- Load the Boston Housing Dataset\n",
        "- Train a Decision Tree Regressor\n",
        "- Print the Mean Squared Error (MSE) and feature importances"
      ],
      "metadata": {
        "id": "JpXW0-M4yCKI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load libraries\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# 1. Load the California Housing Dataset (replacement for Boston Housing)\n",
        "data = fetch_california_housing()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# 2. Split the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# 3. Train a Decision Tree Regressor\n",
        "model = DecisionTreeRegressor(random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# 4. Predict on test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# 5. Calculate Mean Squared Error (MSE)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(\"Mean Squared Error (MSE):\", mse)\n",
        "\n",
        "# 6. Print Feature Importances\n",
        "print(\"\\nFeature Importances:\")\n",
        "for name, importance in zip(data.feature_names, model.feature_importances_):\n",
        "    print(f\"{name}: {importance:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XV3JfENRhIAU",
        "outputId": "dd740109-8544-4fc4-8b26-bae80cd01518"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error (MSE): 0.495235205629094\n",
            "\n",
            "Feature Importances:\n",
            "MedInc: 0.5285\n",
            "HouseAge: 0.0519\n",
            "AveRooms: 0.0530\n",
            "AveBedrms: 0.0287\n",
            "Population: 0.0305\n",
            "AveOccup: 0.1308\n",
            "Latitude: 0.0937\n",
            "Longitude: 0.0829\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 9: Write a Python program to:\n",
        "- Load the Iris Dataset\n",
        "- Tune the Decision Tree’s max_depth and min_samples_split using\n",
        "GridSearchCV\n",
        "- Print the best parameters and the resulting model accuracy"
      ],
      "metadata": {
        "id": "EacU7kOYyCGp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Load the Iris Dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# 2. Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# 3. Define Decision Tree model\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# 4. Parameter grid for tuning\n",
        "param_grid = {\n",
        "    \"max_depth\": [1, 2, 3, 4, 5, None],\n",
        "    \"min_samples_split\": [2, 3, 4, 5, 10]\n",
        "}\n",
        "\n",
        "# 5. GridSearchCV for hyperparameter tuning\n",
        "grid = GridSearchCV(\n",
        "    estimator=dt,\n",
        "    param_grid=param_grid,\n",
        "    cv=5,               # 5-fold cross-validation\n",
        "    scoring=\"accuracy\"\n",
        ")\n",
        "\n",
        "# Fit the grid search\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "# 6. Evaluate best model\n",
        "best_model = grid.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# 7. Print results\n",
        "print(\"Best Parameters:\", grid.best_params_)\n",
        "print(\"Model Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8NUJL75shzq0",
        "outputId": "d1f68af2-bfca-40d4-a6f7-2c2a490a53b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': 4, 'min_samples_split': 2}\n",
            "Model Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 10: Imagine you’re working as a data scientist for a healthcare company that wants to predict whether a patient has a certain disease. You have a large dataset with mixed data types and some missing values.\n",
        "\n",
        "Explain the step-by-step process you would follow to:\n",
        "\n",
        "- Handle the missing values\n",
        "- Encode the categorical features\n",
        "- Train a Decision Tree model\n",
        "- Tune its hyperparameters\n",
        "- Evaluate its performance\n",
        "And describe what business value this model could provide in the real-world\n",
        "setting.\n",
        "\n",
        "\n",
        "##1. Handle the missing values\n",
        "\n",
        "Goal: avoid data leakage, preserve signal in missingness, and keep preprocessing inside CV.\n",
        "\n",
        "Steps:\n",
        "\n",
        "1. Explore missingness\n",
        "\n",
        "   - Percent missing per column, missingness patterns, correlation between missingness and target.\n",
        "\n",
        "2. Classify columns (numeric / categorical / datetime / id).\n",
        "\n",
        "3. Impute inside a pipeline\n",
        "\n",
        "    - Numeric: SimpleImputer(strategy='median') or model-based IterativeImputer if many correlated numerics. Add missing_indicator=True for important features.\n",
        "\n",
        "    - Categorical: SimpleImputer(strategy='constant', fill_value='__MISSING__') (or a dedicated “missing” category).\n",
        "\n",
        "    - Time series / longitudinal: use forward/backward fill or feature engineering (e.g., “days since last visit”).\n",
        "\n",
        "4. Keep imputation inside Pipeline/ColumnTransformer so imputation is fit only on training folds (no leakage).\n",
        "\n",
        "5. If missingness is informative: add explicit binary indicator columns (is_missing_featureX).\n",
        "\n",
        "##2. Encode the categorical features\n",
        "\n",
        "Goal: convert categories to numeric without introducing leakage or artificial ordering.\n",
        "\n",
        "Rules of thumb:\n",
        "\n",
        "- Low-cardinality (≤10 unique): OneHotEncoder(handle_unknown='ignore').\n",
        "\n",
        "- Medium/high-cardinality:\n",
        "\n",
        "    - OrdinalEncoder if no order but tree model can handle integers (beware implied order).\n",
        "\n",
        "    - Target (mean) encoding or frequency encoding for high-cardinality — must be done with CV-safe scheme (e.g., use category_encoders with K-fold target encoding inside the training folds to avoid leakage).\n",
        "\n",
        "- Rare categories: group into 'other' if counts are small.\n",
        "\n",
        "- Unseen categories: ensure encoder handles unknowns (handle_unknown='ignore').\n",
        "\n",
        "Always include encoding in the same pipeline as imputation.\n",
        "\n",
        "##3. Train a Decision Tree model\n",
        "\n",
        "Goal: reproducible, leakage-free training.\n",
        "\n",
        "Steps:\n",
        "\n",
        "1. Train/test split: train_test_split(..., stratify=y, test_size=0.2, random_state=SEED).\n",
        "\n",
        "2. Build preprocessing ColumnTransformer:\n",
        "\n",
        "    - Numeric pipeline: SimpleImputer(median) (+ StandardScaler only if mixing with models that need scaling; not required for trees).\n",
        "\n",
        "    - Categorical pipeline: SimpleImputer(constant='__MISSING__') + OneHotEncoder(handle_unknown='ignore') or other encoder.\n",
        "\n",
        "3. Create Pipeline([('preproc', preproc),\n",
        "       ('clf'DecisionTreeClassifier(random_state=SEED))]).\n",
        "\n",
        "4. Fit the pipeline on training data.\n",
        "\n",
        "5. If class imbalance: either class_weight='balanced' in the classifier or resampling inside CV (e.g., SMOTE via imblearn in a pipeline).\n",
        "\n",
        "##4. Tune hyperparameters\n",
        "\n",
        "Goal: control overfitting and optimize clinically-relevant objectives.\n",
        "\n",
        "Important DecisionTree params:\n",
        "\n",
        "* max_depth\n",
        "\n",
        "* min_samples_split\n",
        "\n",
        "* min_samples_leaf\n",
        "\n",
        "* max_features\n",
        "\n",
        "* criterion (gini or entropy)\n",
        "\n",
        "* class_weight (for imbalance)\n",
        "\n",
        "Tuning approach:\n",
        "\n",
        "1. Choose metric aligned with business cost. For disease detection, often prioritize recall (sensitivity) to avoid missed cases, or use F1 / PR-AUC if false positives are costly too.\n",
        "\n",
        "2. Use cross-validation: StratifiedKFold(n_splits=5).\n",
        "\n",
        "3. Search method: RandomizedSearchCV for large search space; GridSearchCV for small grid.\n",
        "\n",
        "4. Example param grid:\n",
        "\n",
        "  param_grid = {\n",
        "\n",
        "         'clf__max_depth': [3, 5, 8, None],\n",
        "\n",
        "         'clf__min_samples_split': [2, 5, 10],\n",
        "\n",
        "         'clf__min_samples_leaf': [1, 2, 4],\n",
        "\n",
        "         'clf__criterion': ['gini', 'entropy']\n",
        "\n",
        "               }\n",
        "\n",
        "\n",
        "5. Avoid leakage: keep preprocessing inside pipeline passed to GridSearchCV.\n",
        "\n",
        "6. ptionally use nested CV to get an unbiased estimate of performance when reporting.\n",
        "\n",
        "##5. Evaluate performance\n",
        "\n",
        "Goal: evaluate both statistical performance and clinical usefulness.\n",
        "\n",
        "Metrics to compute:\n",
        "\n",
        "* Confusion matrix (TP, FP, TN, FN) — essential for clinical tradeoffs.\n",
        "\n",
        "* Recall (sensitivity) — prioritized if missing disease is costly.\n",
        "\n",
        "* Precision — important if follow-up tests are costly or invasive.\n",
        "\n",
        "* F1-score — balance precision & recall.\n",
        "\n",
        "* ROC AUC and PR AUC (PR AUC is more informative for imbalanced problems).\n",
        "\n",
        "* Calibration (are predicted probabilities meaningful?) — use calibration plots / CalibratedClassifierCV.\n",
        "\n",
        "* Decision-curve analysis or cost-sensitive metrics to choose an operating threshold aligned with business costs.\n",
        "\n",
        "* Subgroup analysis: check performance across age, gender, hospital, etc., to detect bias.\n",
        "\n",
        "Robustness & monitoring:\n",
        "\n",
        "* Evaluate on a held-out test set (or temporal holdout if dataset is time-ordered).\n",
        "\n",
        "* Perform bootstrapping or repeated CV to get confidence intervals for metrics.\n",
        "\n",
        "* Monitor feature stability and concept drift after deployment.\n",
        "\n",
        "\n",
        "##6 Business value in a real-world healthcare setting\n",
        "\n",
        "- Early detection & triage: flag high-risk patients for follow-up tests or specialist review, enabling earlier intervention and improving outcomes.\n",
        "\n",
        "= Resource optimization: prioritize costly diagnostic tests and specialist time for patients most likely to benefit.\n",
        "\n",
        "- Cost savings: avoid unnecessary tests for low-risk patients while catching high-risk cases earlier (reduces downstream costs).\n",
        "\n",
        "- Decision support & auditing: interpretable rules let clinicians understand and validate suggestions, enabling adoption and trust.\n",
        "\n",
        "- Population health management: identify segments with high prevalence and direct preventive programs or screening.\n",
        "\n",
        "- Clinical research: identify important predictors and hypotheses for further study.\n",
        "\n",
        "Risks/limitations to manage:\n",
        "\n",
        "- False negatives (missed disease) have high cost — tune thresholds to prefer sensitivity or add human review for negatives.\n",
        "\n",
        "- Data bias or drift can harm subgroups — perform fairness checks and monitoring.\n",
        "\n",
        "- Regulatory and ethical approvals may be required before clinical use."
      ],
      "metadata": {
        "id": "TkMUeNAOzN54"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0Ecb2kHB2PoA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pKVDQXZo2PkI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ebqDzQm22PhA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QW585DiT2Pdw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XotJXZzXwfFh"
      },
      "outputs": [],
      "source": [
        "##"
      ]
    }
  ]
}